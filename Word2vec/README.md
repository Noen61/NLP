## 词向量训练

利用已经分好词的，给定的中文和英文语料训练对应的词向量。使用的训练词向量的模型为基于负采样（negative sampling）的skip-gram模型。

### 目标函数

给定中心词$\omega_c$的一个背景窗口，生成所有背景词$\omega_o$的事件$P(D=1|\omega_c,\omega_o)$的联合概率。最大化文本序列中所有该事件的联合概率来训练词向量。具体来说，给定⼀个长度为$T$的文本序列，设时间步$t$的词为$\omega^{(t)}$且背景窗口大小为$m$，目标函数即为最大化联合概率：
$$
\prod_{t=1}^{T}\prod_{-m\le{j}\le{m},j\ne{0}}P(D=1|\omega^{(t+j)},\omega^{(t)})
$$

### Skip-gram

Skip-gram模型的基本思想为通过中心词去预测其周围的背景词，经过训练后获得其嵌入矩阵作为词向量。输入向量$x$为对应单词的$one-hot$编码，经过隐层与初始化的词向量表$W^0$相乘后得到单词对应的词向量$v$。$v$再与背景向量表$W^1$相乘后得到所有单词作为该中心词的背景词的概率，经过$softmax$计算后即为$\{y_1,y_2,...,y_c\}$。

### Negative Sampling

负采样针对$softmax$运算导致的每次梯度计算开销过大，将$softmax$函数调整为$sigmoid$函数：
$$
P(D=1|\omega_o,\omega_c)=\sigma(u_o^T{v_c})
$$
目标函数对应的含义由给定中心词求每个词作为背景词的概率，变成了给定中心词求每个词出现在背景窗口中的概率：
$$
\prod_{t=1}^{T}\prod_{-m\le{j}\le{m},j\ne{0}}P(D=1|\omega^{(t+j)},\omega^{(t)})
$$
上述目标函数当仅考虑了正类样本即为背景词样本时，出现的极端情况为当所有词向量相等且值为无穷大时，联合概率才被最大化为$1$。因此需要通过采样并添加负类样本使目标函数更有意义。设背景词出$\omega_o$现在中心词 $\omega_c$的⼀个背景窗口为事件$P$，我们根据分布$P(\omega)$采样$K$个未出现在该背景窗口中的词作为负样本词。负采样下的联合概率为：
$$
\prod_{t=1}^{T}\prod_{-m\le{j}\le{m},j\ne{0}}P(\omega^{(t+j)}|\omega^{(t)})\\
P(\omega^{(t+j)}|\omega^{(t)})=P(D=1|\omega^{(t+j)},\omega^{(t)})\prod_{k=1,\omega_k\sim p(\omega)}^K(D=0|\omega^{(t)},\omega_k)
$$

### 运行

训练：python train.py