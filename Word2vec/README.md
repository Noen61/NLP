## 词向量训练

利用已经分好词的，给定的中文和英文语料训练对应的词向量。使用的训练词向量的模型为基于负采样（negative sampling）的skip-gram模型。

### 目标函数

给定中心词的一个背景窗口，生成所有背景词的事件P的联合概率。最大化文本序列中所有该事件的联合概率来训练词向量，目标函数即为最大化联合概率。

### Skip-gram

Skip-gram模型的基本思想为通过中心词去预测其周围的背景词，经过训练后获得其嵌入矩阵作为词向量。输入向量x为对应单词的one-hot编码，经过隐层与初始化的词向量表相乘后得到单词对应的词向量v。v再与背景向量表相乘后得到所有单词作为该中心词的背景词的概率，经过softmax计算后即输出logits。

### Negative Sampling

负采样针对softmax运算导致的每次梯度计算开销过大，将softmax函数调整为sigmoid函数。目标函数对应的含义由给定中心词求每个词作为背景词的概率，变成了给定中心词求每个词出现在背景窗口中的概率。上述目标函数当仅考虑了正类样本即为背景词样本时，出现的极端情况为当所有词向量相等且值为无穷大时，联合概率才被最大化为1。因此需要通过采样并添加负类样本使目标函数更有意义。

### 运行

训练：python train.py